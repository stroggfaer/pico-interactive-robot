# AI –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–æ–±–æ—Ç —É–ø—Ä–∞–≤–ª—è–µ—Ç —á–µ—Ä–µ–∑ –ø–æ—Ä—Ç COM5 RP2040 Raspberry Pi Pico Bootloader v3.0 v1.0
# –î–ª—è —Ä–∞–±–æ—Ç—ã —Å –≥–æ–ª–æ—Å–æ–º –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ gtts
# –î–ª—è —Ä–∞–±–æ—Ç—ã —Å –∞—É–¥–∏–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ pydub
# –î–ª—è —Ä–∞–±–æ—Ç—ã —Å —Å–µ—Ç—å—é –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ requests
# –î–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø–æ—Ç–æ–∫–∞–º–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ threading
# –î–ª—è —Ä–∞–±–æ—Ç—ã —Å UUID –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ uuid
# –î–ª—è —Ä–∞–±–æ—Ç—ã —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ os
# –î–ª—è —Ä–∞–±–æ—Ç—ã —Å JSON –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ json
# –î–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ–º —Ä–µ—á–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ speech_recognition
# GIThab: https://github.com/stroggfaer/pico-interactive-robot/tree/main
# –ê–≤—Ç–æ—Ä: Rendzhi
# command_ai.py
import serial
import time
import json
import requests
import random
import re
import speech_recognition as sr
import threading
import os
import uuid
from gtts import gTTS
import pygame
import wave
import contextlib
from pydub import AudioSegment
import scipy.io.wavfile as wavfile
import numpy as np
from requests.exceptions import HTTPError

HAS_MODE_AI = False  # True ‚Äî AI/–º–∏–∫—Ä–æ—Ñ–æ–Ω, False ‚Äî —Ç–æ–ª—å–∫–æ AUDIO_TEXT_MAP
#Test –≠–º–æ—Ü–∏—è;
AUDIO_TEXT_MAP = [
    {
        "text": "–ü—Ä–∏–≤–µ—Ç! –Ø –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ä–æ–±–æ—Ç. –Ø —É–º–µ—é –≥–æ–≤–æ—Ä–∏—Ç—å –∏ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å —ç–º–æ—Ü–∏–∏.",
        "emotion": "talking",
        "talking_emotion": "neutral",
        "mouth_speed": 0.3,
        "duration": 6.0,
        "anim_duration": 1
    },
    # –•–∞—Ä–∞–∫—Ç–µ—Ä —ç–º–æ—Ü–∏–π –¥–ª—è AUDIO_TEXT_MAP (—Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∞)
    { "text": "–ì—Ä—É—Å—Ç–Ω—ã–π", "emotion": "sad", "duration": 5.0, "anim_duration": 5},
    { "text": "–í–µ—Å–µ–ª—ã–π", "emotion": "happy", "duration": 5.0, "anim_duration": 5},
    { "text": "–°—Ç—Ä–∞—à–Ω—ã–π", "emotion": "scary", "duration": 5.0, "anim_duration": 5},
    { "text": "–£–¥–∏–≤–ª–µ–Ω–Ω—ã–π", "emotion": "surprise", "duration": 5.0, "anim_duration": 5},
    { "text": "–°–º—É—â–µ–Ω–Ω—ã–π", "emotion": "embarrassed", "duration": 5.0, "anim_duration": 5},
    { "text": "–í–ª—é–±–ª–µ–Ω–Ω—ã–π", "emotion": "smile_love", "duration": 5.0, "anim_duration": 5},
    # –ü—Ä–∏–º–µ—Ä—ã talking_emotion —Å —Ä–∞–∑–Ω—ã–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏
    {
        "text": "–Ø –º–æ–≥—É –≥–æ–≤–æ—Ä–∏—Ç—å —Å —Ä–∞–∑–Ω—ã–º–∏ —ç–º–æ—Ü–∏—è–º–∏. –°–µ–π—á–∞—Å —è –≥–æ–≤–æ—Ä—é —Å–µ—Ä–¥–∏—Ç–æ!",
        "emotion": "talking",
        "talking_emotion": "angry",
        "mouth_speed": 0.7,
        "duration": 4.0,
        "anim_duration": 1
    },
    {
        "text": "–ü–æ—Å–º–æ—Ç—Ä–∏, –∫–∞–∫ —è —Ö–∏—Ç—Ä–æ —É–ª—ã–±–∞—é—Å—å, –∫–æ–≥–¥–∞ –≥–æ–≤–æ—Ä—é!",
        "emotion": "talking",
        "talking_emotion": "smile_tricky",
        "mouth_speed": 0.5,
        "duration": 4.0,
        "anim_duration": 1
    },
    {
        "text": "–Ø –º–æ–≥—É –±—ã—Ç—å –æ—á–µ–Ω—å –æ–∑–æ—Ä–Ω—ã–º, –∫–æ–≥–¥–∞ —Ä–∞–∑–≥–æ–≤–∞—Ä–∏–≤–∞—é!",
        "emotion": "talking",
        "talking_emotion": "tricky",
        "mouth_speed": 0.6,
        "duration": 4.0,
        "anim_duration": 1
    },
    {
        "text": "–Ø –º–æ–≥—É –≥–æ–≤–æ—Ä–∏—Ç—å —Å —à–∏—Ä–æ–∫–æ–π —É–ª—ã–±–∫–æ–π!",
        "emotion": "talking",
        "talking_emotion": "smile",
        "mouth_speed": 0.8,
        "duration": 4.0,
        "anim_duration": 1
    },
    {
        "text": "–•–∞-—Ö–∞-—Ö–∞! –Ø —Å–º–µ—é—Å—å –∏ –≥–æ–≤–æ—Ä—é –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ!",
        "emotion": "talking",
        "talking_emotion": "ha",
        "mouth_speed": 1.0,
        "duration": 3.0,
        "anim_duration": 1
    },
    {
        "text": "–¢–µ–ø–µ—Ä—å —Ç—ã –º–æ–∂–µ—à—å –≥–æ–≤–æ—Ä–∏—Ç—å —Å–æ –º–Ω–æ–π —á–µ—Ä–µ–∑ –º–∏–∫—Ä–æ—Ñ–æ–Ω! –ü—Ä–æ—Å—Ç–æ —Å–∫–∞–∂–∏ —á—Ç–æ-–Ω–∏–±—É–¥—å, –∏ —è –æ—Ç–≤–µ—á—É —Ç–µ–±–µ –≥–æ–ª–æ—Å–æ–º –∏ —ç–º–æ—Ü–∏—è–º–∏. –£–¥–∞—á–∏)",
        "emotion": "talking",
        "talking_emotion": "neutral",
        "mouth_speed": 0.7,
        "duration": 5.0,
        "anim_duration": 1
    },
]

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
OPENROUTER_API_URL = "https://openrouter.ai/api/v1/chat/completions"
OPENROUTER_API_KEY = ""
MODEL_NAME = "deepseek/deepseek-chat" #"nousresearch/deephermes-3-mistral-24b-preview:free"
ROBOT_VOICE_EFFECT = 70 # –ß–µ–º –≤—ã—à–µ –∑–Ω–∞—á–µ–Ω–∏–µ, —Ç–µ–º "—Ç–æ–Ω—å—à–µ" –∏ –±–æ–ª–µ–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –≥–æ–ª–æ—Å

TTS_MODE = "gtts"
ROBOT_VOICE_LANG = "ru"
TEMP_AUDIO_DIR = os.path.join(os.path.dirname(__file__), "temp") # –ü—É—Ç—å –∫ –ø–∞–ø–∫–µ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤


if not os.path.exists(TEMP_AUDIO_DIR):
    os.makedirs(TEMP_AUDIO_DIR)

pygame.mixer.init()

ser = serial.Serial('COM5', 115200, timeout=1)
time.sleep(2)
if HAS_MODE_AI:
    recognizer = sr.Recognizer()

TALKING_EMOTIONS = ["talking"]
FINAL_EMOTIONS = ["neutral"]

def get_wav_duration(file_path):
    with contextlib.closing(wave.open(file_path, 'r')) as f:
        frames = f.getnframes()
        rate = f.getframerate()
        duration = frames / float(rate)
        return duration

def apply_robot_effect(wav_path):
    sample_rate, data = wavfile.read(wav_path)
    t = np.linspace(0, len(data) / sample_rate, num=len(data))
    modulator = np.sin(2 * np.pi * ROBOT_VOICE_EFFECT * t)
    robotized = (data * modulator).astype(data.dtype)
    robot_wav_path = wav_path.replace('.wav', '_robot.wav')
    wavfile.write(robot_wav_path, sample_rate, robotized)
    return robot_wav_path

def speak_and_send(text, ser, emotion, talking_emotion="neutral", intensity=1.0, volume=1.0, mouth_speed=0.2, sync=False, send_serial=True):
    if not text.strip():
        return

    def process():
        try:
            tts = gTTS(text=text, lang=ROBOT_VOICE_LANG, slow=False)
            unique_id = uuid.uuid4().hex
            mp3_path = os.path.join(TEMP_AUDIO_DIR, f"robot_voice_{unique_id}.mp3")
            wav_path = os.path.join(TEMP_AUDIO_DIR, f"robot_voice_{unique_id}.wav")
            tts.save(mp3_path)

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º MP3 –≤ WAV
            audio = AudioSegment.from_mp3(mp3_path)
            audio.export(wav_path, format="wav")

            wav_path = apply_robot_effect(wav_path)

            real_duration = get_wav_duration(wav_path)

            if send_serial:
                json_command = json.dumps({
                    "emotion": emotion if emotion in TALKING_EMOTIONS else "talking",
                    "talking_emotion": talking_emotion,
                    "text": text,
                    "mouth_speed": mouth_speed,
                    "duration": real_duration,
                    "intensity": intensity,
                    "volume": volume
                })
                ser.write((json_command + "\r\n").encode('utf-8'))
                print(f"Sent: {json_command}")

            pygame.mixer.music.load(wav_path)
            pygame.mixer.music.play()

            while pygame.mixer.music.get_busy():
                time.sleep(0.1)

            pygame.mixer.music.stop()
            pygame.mixer.quit()
            pygame.mixer.init()

            if os.path.exists(mp3_path):
                os.remove(mp3_path)
            if os.path.exists(wav_path):
                os.remove(wav_path)

            if send_serial:
                try:
                    response = ser.readline().decode('utf-8').strip()
                    print(f"Response: {response}")
                except Exception as e:
                    print(f"[ERROR] Reading response: {e}")

        except Exception as e:
            print(f"[gTTS ERROR] {e}")

    if sync:
        process()
    else:
        threading.Thread(target=process).start()

def detect_emotion_simple(text):
    if not text:
        return "neutral"
        
    text_lower = text.lower()
    emotion_triggers = {
        "smile": ["—Ö–∞", "–∞—Ö–∞—Ö", "–ª–æ–ª", "—Å–º–µ—à–Ω–æ", "–≤–µ—Å–µ–ª–æ", "–∑–∞–±–∞–≤–Ω–æ", "—Ö–µ—Ö–µ"],
        "smile_love": ["–ª—é–±–ª—é", "–æ–±–æ–∂–∞—é", "–º–∏–ª–∞—è", "‚ù§Ô∏è", "‚ô•Ô∏è", "–º–∏–ª—ã–π", "–ø—Ä–µ–ª–µ—Å—Ç—å", "–∫—Ä–∞—Å–æ—Ç–∞"],
        "scary": ["–±–æ—é—Å—å", "—Å—Ç—Ä–∞—à–Ω–æ", "—É–∂–∞—Å", "–∂—É—Ç–∫–æ", "–∫–æ—à–º–∞—Ä", "–ø–∞–Ω–∏–∫–∞"],
        "sad": ["–≥—Ä—É—Å—Ç–Ω–æ", "–ø–µ—á–∞–ª—å–Ω–æ", "–∂–∞–ª—å", "—Ç–æ—Å–∫–∞", "–≥—Ä—É—Å—Ç—å", "–ø–µ—á–∞–ª—å", "–ø–ª–æ—Ö–æ"],
        "surprise": ["—É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ", "–æ–≥–æ", "–≤–∞—É", "–Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ", "–ø–æ—Ä–∞–∑–∏—Ç–µ–ª—å–Ω–æ", "–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ", "!"],
        "embarrassed": ["—Å—Ç—ã–¥–Ω–æ", "—Å–º—É—â–∞—é—Å—å", "–Ω–µ–ª–æ–≤–∫–æ", "–∏–∑–≤–∏–Ω–∏", "–ø—Ä–æ—Å—Ç–∏", "—Å—Ç–µ—Å–Ω—è—é—Å—å"],
        "angry_talking": ["–∑–ª–æ–π", "–∑–ª–æ—Å—Ç—å", "—Ä–∞–∑–¥—Ä–∞–∂–∞–µ—Ç", "–±–µ—Å–∏—Ç", "–Ω–µ–Ω–∞–≤–∏–∂—É", "–≥–Ω–µ–≤"],
        "happy": ["—Ä–∞–¥–æ—Å—Ç—å", "—Å—á–∞—Å—Ç—å–µ", "–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ", "—Å—É–ø–µ—Ä", "–æ—Ç–ª–∏—á–Ω–æ", "–∫–ª–∞—Å—Å", "–∫—Ä—É—Ç–æ"]
    }

    if "?" in text:
        for emotion, triggers in emotion_triggers.items():
            if any(trigger in text_lower for trigger in triggers):
                return emotion
        return "talking"

    if "!" in text:
        for emotion, triggers in emotion_triggers.items():
            if any(trigger in text_lower for trigger in triggers):
                return emotion
        return "happy"

    max_triggers = 0
    detected_emotion = "neutral"
    
    for emotion, triggers in emotion_triggers.items():
        trigger_count = sum(1 for trigger in triggers if trigger in text_lower)
        if trigger_count > max_triggers:
            max_triggers = trigger_count
            detected_emotion = emotion

    if max_triggers > 0:
        return detected_emotion

    if text.strip():
        return "talking"

    return "neutral"

def get_emotion_from_context(text):
    emotion = detect_emotion_simple(text)
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º talking_emotion –Ω–∞ –æ—Å–Ω–æ–≤–µ emotion (–æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –¥–æ–ø—É—Å—Ç–∏–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)
    allowed_talking_emotions = ["neutral", "angry", "smile_tricky", "tricky", "smile", "ha"]
    talking_emotion = emotion if emotion in allowed_talking_emotions else "neutral"
    prompt = f"""
    –¢—ã ‚Äî –∑–ª–æ–π –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–π —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≥–æ–≤–æ—Ä–∏—Ç —Å —ç–º–æ—Ü–∏–µ–π: {emotion}.
    –û—Ç–≤–µ—Ç—å –∫–æ—Ä–æ—Ç–∫–æ –∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º –≤ —ç—Ç–æ–º –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–∏.
    –§—Ä–∞–∑–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: \"{text}\"

    –û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON –±–µ–∑ –ª–∏—à–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞:
    {{
        \"emotion\": \"{emotion}\",
        \"text\": \"—Ç–≤–æ–π –æ—Ç–≤–µ—Ç –∑–¥–µ—Å—å\"
    }}
    """

    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json"
    }

    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "user", "content": prompt}
        ]
    }

    try:
        response = requests.post(OPENROUTER_API_URL, headers=headers, json=payload)
        response.raise_for_status()
        message = response.json()["choices"][0]["message"]["content"]
        print(f"ü§ñ AI response: {message}")
        match = re.search(r'\{.*?\}', message, re.DOTALL)
        if match:
            emotion_data = json.loads(match.group())
            if "text" in emotion_data:
                result = {"emotion": emotion, "talking_emotion": talking_emotion, "text": emotion_data["text"]}
                print(f"‚ú® Final response: {result}")
                return result
    except HTTPError as e:
        if hasattr(response, 'status_code') and response.status_code == 429:
            print("–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ AI. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")
            return {"emotion": emotion, "talking_emotion": "angry", "text": "–õ–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ OpenRouter –ø—Ä–µ–≤—ã—à–µ–Ω. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."}
        else:
            print(f"üî¥ –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ OpenRouter: {e}")

    return {"emotion": emotion, "talking_emotion": "tricky", "text": "–ß—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫, –ø–æ–ø—Ä–æ–±—É–π –ø–æ–≤—Ç–æ—Ä–∏—Ç—å!"}

def recognize_speech():
    try:
        with sr.Microphone() as source:
            print("üéôÔ∏è –ì–æ–≤–æ—Ä–∏ –≤ –º–∏–∫—Ä–æ—Ñ–æ–Ω...")
            recognizer.adjust_for_ambient_noise(source, duration=1)
            audio = recognizer.listen(source, timeout=6, phrase_time_limit=12)
            text = recognizer.recognize_google(audio, language='ru-RU')
            print(f"üü¢ –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: {text}")
            return text
    except Exception as e:
        print(f"üî¥ –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–∏: {e}")
        return ""

def change_speed(wav_path, speed=1.2):
    sound = AudioSegment.from_wav(wav_path)
    sound_with_altered_frame_rate = sound._spawn(sound.raw_data, overrides={
         "frame_rate": int(sound.frame_rate * speed)
      })
    sound_with_altered_frame_rate = sound_with_altered_frame_rate.set_frame_rate(sound.frame_rate)
    new_path = wav_path.replace('.wav', f'_speed{speed}.wav')
    sound_with_altered_frame_rate.export(new_path, format="wav")
    return new_path

initial = ser.readline().decode('utf-8').strip()
print(f"Initial: {initial}")
ser.write((json.dumps({"emotion": "neutral", "duration": 2.0, "intensity": 1.0, "volume": 0.3}) + "\r\n").encode('utf-8'))

if HAS_MODE_AI:
    input_choice = input("üéôÔ∏è –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–∏–∫—Ä–æ—Ñ–æ–Ω? (–¥–∞/–Ω–µ—Ç): ").lower()
    use_mic = input_choice == "–¥–∞"
else:
    use_mic = False

if not HAS_MODE_AI:
    for audio_entry in AUDIO_TEXT_MAP:
        text = audio_entry.get("text", "–¢–µ–∫—Å—Ç –Ω–µ –∑–∞–¥–∞–Ω")
        emotion = audio_entry["emotion"]
        talking_emotion = audio_entry.get("talking_emotion", "neutral")
        mouth_speed = audio_entry.get("mouth_speed", 1.5)
        anim_duration = audio_entry.get("anim_duration", 0.5)
        duration = audio_entry.get("duration", 1.0)

        try:
            # 1. –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∞—É–¥–∏–æ—Ñ–∞–π–ª –∏ —É–∑–Ω–∞—Ç—å duration
            tts = gTTS(text=text, lang=ROBOT_VOICE_LANG, slow=False)
            unique_id = uuid.uuid4().hex
            mp3_path = os.path.join(TEMP_AUDIO_DIR, f"robot_voice_{unique_id}.mp3")
            wav_path = os.path.join(TEMP_AUDIO_DIR, f"robot_voice_{unique_id}.wav")
            tts.save(mp3_path)
            audio = AudioSegment.from_mp3(mp3_path)
            audio.export(wav_path, format="wav")
            wav_path = apply_robot_effect(wav_path)
            real_duration = get_wav_duration(wav_path)

            # 2. –û—Ç–ø—Ä–∞–≤–∏—Ç—å –∫–æ–º–∞–Ω–¥—É —Å duration
            if emotion == "talking":
                send_duration = real_duration
            else:
                send_duration = duration
            json_command = json.dumps({
                "emotion": emotion,
                "talking_emotion": talking_emotion,
                "text": text,
                "mouth_speed": mouth_speed,
                "duration": send_duration,
                "anim_duration": anim_duration,
                "intensity": 1.0,
                "volume": 1.0
            })
            ser.write((json_command + "\r\n").encode('utf-8'))
            print(f"Sent___: {json_command}")

            # 3. –°—Ä–∞–∑—É –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ –∞—É–¥–∏–æ (–±–µ–∑ sleep)
            pygame.mixer.music.load(wav_path)
            pygame.mixer.music.play()
            while pygame.mixer.music.get_busy():
                time.sleep(0.1)
            pygame.mixer.music.stop()
            pygame.mixer.quit()
            pygame.mixer.init()

            # –ü–æ—Å–ª–µ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –∞—É–¥–∏–æ, —É–¥–∞–ª–∏—Ç—å –≤—Å–µ —Ñ–∞–π–ª—ã –∏–∑ TEMP_AUDIO_DIR
            try:
                for fname in os.listdir(TEMP_AUDIO_DIR):
                    fpath = os.path.join(TEMP_AUDIO_DIR, fname)
                    if os.path.isfile(fpath):
                        os.remove(fpath)
            except Exception as e:
                print(f"[ERROR] Cleaning temp dir: {e}")

            # 5. –î–æ–∂–¥–∞—Ç—å—Å—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç RP2040 (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
            try:
                response = ser.readline().decode('utf-8').strip()
                print(f"Response: {response}")
            except Exception as e:
                print(f"[ERROR] Reading response: {e}")

            # 6. –ü–∞—É–∑–∞ –º–µ–∂–¥—É —ç–º–æ—Ü–∏—è–º–∏
            time.sleep(duration)
        except Exception as e:
            print(f"[ERROR] Audio/Emotion sync: {e}")

    ser.close()
    print("–ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
    exit(0)

while True:
    if HAS_MODE_AI and use_mic:
        user_input = recognize_speech()
    else:
        user_input = input("> ")

    if user_input.lower() == "q":
        break
    if not user_input:
        continue

    emotion_data = get_emotion_from_context(user_input)
    emotion = emotion_data["emotion"]
    talking_emotion = emotion_data.get("talking_emotion", "neutral")
    llama_response = emotion_data["text"]

    print(f"\nüü¢ –≠–º–æ—Ü–∏—è: {emotion}")
    print(f"üü¢ –û—Ç–≤–µ—Ç: {llama_response}")
    mouth_speed = max(0.1, min(0.5, 0.1 + (len(llama_response) / 50)))
    if llama_response:
        speak_and_send(llama_response, ser, emotion, talking_emotion=talking_emotion, intensity=1.0, volume=1.0, mouth_speed=mouth_speed)
        final_emotion = random.choice(FINAL_EMOTIONS)
        ser.write((json.dumps({"emotion": final_emotion, "duration": 2.0, "intensity": 1.0, "volume": 0.2}) + "\r\n").encode('utf-8'))
    else:
        ser.write((json.dumps({"emotion": emotion, "duration": 2.0, "intensity": 1.0, "volume": 1.22}) + "\r\n").encode('utf-8'))

    response = ser.readline().decode('utf-8').strip()
    print(f"Response: {response}")
ser.close()
print("–ü—Ä–æ–≥—Ä–∞–º–º–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")